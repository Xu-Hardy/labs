---
title:  Amazon Bedrock Python SDK 完成视频风格迁移
---

本实验演示如何使用Amazon Bedrock服务，结合Stable Diffusion XL (SDXL) 1.0 模型从视频生成新的视频。步骤包括将视频或者动图分解为帧，通过提示词生成新图像帧，并将这些帧重新合成为新动图。

1. **选择并准备视频**
   - 将需要处理的视频复制到 `source_videos` 目录中。

2. **使用FFmpeg将视频分割为帧**
   - 使用FFmpeg工具将视频分解为单独的JPEG帧，并将这些帧保存到 `source/frames` 目录中。

3. **选择帧序列**
   - 从分解的帧中选择一组连续的帧，复制到一个新的目录中，用于后续的动画处理。

4. **生成新的帧**
   - 使用之前的图像到图像的生成方法，循环遍历选择的帧目录，根据提示词生成新的图像帧，并保存到指定输出目录中。

5. **检查并清理帧**（可选）
   - 检查生成的帧，删除任何异常或不符合要求的帧。

6. **将帧重新组合成视频**
   - 使用FFmpeg将生成的图像帧重新组合成视频动画（可选择生成GIF或MP4格式）。

#### 实验步骤

1. **环境依赖**
   需要以下Python库：
   - `boto3`: 用于与AWS Bedrock服务交互
   - `PIL` (Python Imaging Library): 用于处理图像
   - `ffmpeg-python`: 用于处理视频和图像帧
   - `base64`, `io`, `json`, `logging`, `os`, `time`: 标准库模块

   通过以下命令安装依赖项：
   ```bash
   sudo apt install ffmpeg -y
   pip3 install boto3 pillow ffmpeg-python
   ```
   
2. **模型设置与路径配置**
   - `MODEL_ID`: 模型的ID。
   - `SOURCE_VIDEO`: 源视频路径。
   - `SOURCE_VIDEO_FRAMES`: 保存分解帧的路径。
   - `GENERATED_VIDEO_FRAMES`: 保存生成的新图像帧的路径。
   - `GENERATED_VIDEO`: 最终合成的视频输出路径。

   <video src="/Users/xu/Desktop/bedrock SD/灵魂画手 —— 基于 Amazon Bedrock 的动图生成与风格迁移/图片/dance.mp4"></video>
   
   示例代码：
   ```python
   SOURCE_IMAGES = "./source/img"
   SOURCE_VIDEOS = "./source/videos"
   SOURCE_FRAMES = "./source/frames"
   
   GENERATED_IMAGES = "./generate/img"
   GENERATED_FRAMES = "./generate/frames"
   GENERATED_VIDEOS = "./generate/videos"
   
   video_name = "red_squirrel.mp4"
   SOURCE_VIDEO = f"{SOURCE_VIDEOS}/{video_name}"
   SOURCE_VIDEO_FRAMES = f"{SOURCE_FRAMES}/{video_name}"
   GENERATED_VIDEO_FRAMES = f"{GENERATED_FRAMES}/{video_name}"
   GENERATED_VIDEO = f"{GENERATED_VIDEOS}/{video_name}"
   
   if not os.path.exists(SOURCE_VIDEO_FRAMES):
       os.makedirs(SOURCE_VIDEO_FRAMES)
   if not os.path.exists(GENERATED_VIDEO_FRAMES):
       os.makedirs(GENERATED_VIDEO_FRAMES)
   ```
   
3. **视频帧提取**
   使用`ffmpeg`将源视频分解为图像帧，保存到`SOURCE_VIDEO_FRAMES`目录：
   ```python
   ffmpeg.input(SOURCE_VIDEO).output(
       f"{SOURCE_VIDEO_FRAMES}/frame%d.jpg",
       start_number=1,
       qscale=5
   ).overwrite_output().run(quiet=False)
   ```

4. **图像到图像请求类**
   `ImageToImageRequest`类封装图像生成所需的参数，正负提示词用于指导生成帧：
   ```python
   imageToImageRequest = ImageToImageRequest(
       image_width=1280,
       image_height=704,
       positive_prompt="red squirrel with red eyes, in the style of Pixar animation",
       negative_prompt="out of frame, lowres, text, error, cropped, worst quality...",
       image_strength=0.4,
       cfg_scale=15,
       clip_guidance_preset="SLOWEST",
       sampler="K_DPMPP_2M",
       samples=1,
       seed=123234343,
       steps=25,
       style_preset=StylesPresets.ANIME.value,
   )
   ```

5. **逐帧生成新图像**
   遍历从源视频提取的帧，对每一帧生成新的图像，保存到`GENERATED_VIDEO_FRAMES`目录：
   
   ```python
   for i in range(160, 200, 2):
       image_to_image_request(
           imageToImageRequest,
           f"{SOURCE_VIDEO_FRAMES}/frame{i}.jpg",
           GENERATED_VIDEO_FRAMES,
       )
   ```
   
6. **合成新视频**
   使用`ffmpeg`将生成的图像帧合成为新视频：
   ```python
   ffmpeg.input(
       f"{GENERATED_VIDEO_FRAMES}/*.jpg",
       pattern_type="glob",
       framerate=9
   ).output(GENERATED_VIDEO).run(overwrite_output=True)
   ```

7. **完整代码运行**
   将以上所有步骤组合成完整的实验流程，从视频到视频的图像生成并合成新视频。

#### 实验结果
实验运行后，生成的新视频将保存在 `./generate/videos/` 目录中，文件名为源视频的名称，例如 `red_squirrel.mp4`。

<video src="/Users/xu/Desktop/bedrock SD/灵魂画手 —— 基于 Amazon Bedrock 的动图生成与风格迁移/图片/dacnce_new.mp4"></video>



**完整代码如下：**

```python
import ffmpeg
import base64
import io
from enum import Enum, unique
import json
import logging
import boto3
from PIL import Image
import time
import os
from botocore.exceptions import ClientError



SOURCE_IMAGES = "./source/img"
SOURCE_VIDEOS = "source/videos"
SOURCE_FRAMES = "./source/frames"

GENERATED_IMAGES = "./generate/img"
GENERATED_FRAMES = "./generate/frames"
GENERATED_VIDEOS = "./generate/videos"

video_name = "dance.mp4"

MODEL_ID = "stability.stable-diffusion-xl-v1"
SOURCE_VIDEO = f"{SOURCE_VIDEOS}/{video_name}"
SOURCE_VIDEO_FRAMES = f"{SOURCE_FRAMES}/{video_name}"
GENERATED_VIDEO_FRAMES = f"{GENERATED_FRAMES}/{video_name}"
GENERATED_VIDEO = f"{GENERATED_VIDEOS}/{video_name}"

if not os.path.exists(SOURCE_VIDEO_FRAMES):
    os.makedirs(SOURCE_VIDEO_FRAMES)
if not os.path.exists(GENERATED_VIDEO_FRAMES):
    os.makedirs(GENERATED_VIDEO_FRAMES)


# Set up logging for notebook environment
logger = logging.getLogger(__name__)
if logger.hasHandlers():
    logger.handlers.clear()
handler = logging.StreamHandler()
logger.addHandler(handler)
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger.setLevel(logging.INFO)

class ImageToImageRequest:
    """
    Class for handling image to image request parameters.
    """

    def __init__(
        self,
        image_width,
        image_height,
        positive_prompt,
        negative_prompt,
        init_image_mode="IMAGE_STRENGTH",
        image_strength=0.5,
        cfg_scale=7,
        clip_guidance_preset="SLOWER",
        sampler="K_DPMPP_2M",
        samples=1,
        seed=1,
        steps=30,
        style_preset="photographic",
        extras=None,
    ):
        self.image_width = image_width
        self.image_height = image_height
        self.positive_prompt = positive_prompt
        self.negative_prompt = negative_prompt
        self.init_image_mode = init_image_mode
        self.image_strength = image_strength
        self.cfg_scale = cfg_scale
        self.clip_guidance_preset = clip_guidance_preset
        self.sampler = sampler
        self.samples = samples
        self.seed = seed
        self.steps = steps
        self.style_preset = style_preset
        self.extras = extras


@unique
class StylesPresets(Enum):
    """
    Enumerator for SDXL style presets.
    """

    THREE_D_MODEL = "3d-model"
    ANALOG_FILM = "analog-film"
    ANIME = "anime"
    CINEMATIC = "cinematic"
    COMIC_BOOK = "comic-book"
    DIGITAL_ART = "digital-art"
    ENHANCE = "enhance"
    FANTASY_ART = "fantasy-art"
    ISOMETRIC = "isometric"
    LINE_ART = "line-art"
    LOW_POLY = "low-poly"
    MODELING_COMPOUND = "modeling-compound"
    NEON_PUNK = "neon-punk"
    ORIGAMI = "origami"
    PHOTOGRAPHIC = "photographic"
    PIXEL_ART = "pixel-art"
    TILE_TEXTURE = "tile-texture"


class ImageError(Exception):
    """
    Custom exception for errors returned by SDXL 1.0.
    """

    def __init__(self, message):
        self.message = message

def image_to_image_request(
    imageToImageRequest,
    source_image,
    generated_images,
):
    """
    Entrypoint for SDXL example.
    Args:
        imageToImageRequest (ImageToImageRequest): The image to image request to use.
        generated_images (str): The directory to save the generated images to.
        source_image (str): The source image to use.
    """

    # Read source image from file and encode as base64 strings
    image = Image.open(source_image)
    new_image = image.resize(
        (imageToImageRequest.image_width, imageToImageRequest.image_height)
    )

    new_image.save(f"{source_image[:-4]}_tmp.jpg")

    with open(f"{source_image[:-4]}_tmp.jpg", "rb") as image_file:
        init_image = base64.b64encode(image_file.read()).decode("utf8")

    # Build request body
    body = json.dumps(
        {
            "text_prompts": [
                {"text": imageToImageRequest.positive_prompt, "weight": 1},
                {"text": imageToImageRequest.negative_prompt, "weight": -1},
            ],
            "init_image": init_image,
            "init_image_mode": imageToImageRequest.init_image_mode,
            "image_strength": imageToImageRequest.image_strength,
            "cfg_scale": imageToImageRequest.cfg_scale,
            "clip_guidance_preset": imageToImageRequest.clip_guidance_preset,
            "sampler": imageToImageRequest.sampler,
            "samples": imageToImageRequest.samples,
            "seed": imageToImageRequest.seed,
            "steps": imageToImageRequest.steps,
            "style_preset": imageToImageRequest.style_preset,
        }
    )

    try:
        logger.info(f"Source image: {source_image}")
        image_bytes = generate_image_from_image(model_id=MODEL_ID, body=body)
        image = Image.open(io.BytesIO(image_bytes))
        epoch_time = int(time.time())
        # generated_image_path = f"{generated_images}/image_{epoch_time}.jpg"
        generated_image_path = f"{GENERATED_VIDEO_FRAMES}/image_{epoch_time}_{imageToImageRequest.seed}_{imageToImageRequest.sampler}_{imageToImageRequest.image_strength}_{imageToImageRequest.cfg_scale}_{imageToImageRequest.steps}_{imageToImageRequest.style_preset}.jpg"
        logger.info(f"Generated image: {generated_image_path}")
        # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#png
        # image.save(generated_image_path, format="PNG", compress_level=1)
        # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#jpeg-saving
        image.save(generated_image_path, format="JPEG", quality=95)

    except ClientError as err:
        message = err.response["Error"]["Message"]
        logger.error("A client error occurred: %s", message)
    except ImageError as err:
        logger.error(err.message)

    else:
        logger.info(f"Finished generating image with SDXL model {MODEL_ID}.")

def generate_image_from_image(model_id, body):
    """
    Generate an image using SDXL 1.0 on demand.
    Args:
        model_id (str): The model ID to use.
        body (str) : The request body to use.
    Returns:
        image_bytes (bytes): The image generated by the model.
    """

    logger.info("Generating image with SDXL model %s", model_id)

    bedrock = boto3.client(service_name="bedrock-runtime")

    accept = "application/json"
    content_type = "application/json"

    response = bedrock.invoke_model(
        body=body, modelId=model_id, accept=accept, contentType=content_type
    )
    response_body = json.loads(response.get("body").read())
    logger.info(f"Bedrock result: {response_body['result']}")

    base64_image = response_body.get("artifacts")[0].get("base64")
    base64_bytes = base64_image.encode("ascii")
    image_bytes = base64.b64decode(base64_bytes)

    finish_reason = response_body.get("artifacts")[0].get("finishReason")

    if finish_reason == "ERROR" or finish_reason == "CONTENT_FILTERED":
        raise ImageError(f"Image generation error. Error code is {finish_reason}")

    logger.info("Successfully generated image with the SDXL 1.0 model %s", model_id)

    return image_bytes
# step1
(
    ffmpeg.input(
        SOURCE_VIDEO,
    )
    .output(
        f"{SOURCE_VIDEO_FRAMES}/frame%d.jpg",
        start_number=1,
        qscale=5,
    )
    .overwrite_output()
    .run(quiet=False)
)

POSITIVE_PROMPT="anime-style female dancer, caucasian, brightly colored hair, dressed in bright vibrant colors clothing, many girls, white background, empty hands"

NEGATIVE_PROMPT = "out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"


# Maximum resolution for SDXL v1.0 is 1,048,576 pixels/frame (e.g., 1024 x 1024)
# Pixel width and height of the image to generate must be a multiple of 64
# Current video frames: 1280 W x 720 H = 921,600 pixels
# 1280/64 = 20
# 720/64 = 11.25 > 11

imageToImageRequest = ImageToImageRequest(
    image_width=(20 * 64),
    image_height=(11 * 64),
    positive_prompt=POSITIVE_PROMPT,
    negative_prompt=NEGATIVE_PROMPT,
    init_image_mode="IMAGE_STRENGTH",
    image_strength=0.4,
    cfg_scale=15,
    clip_guidance_preset="SLOWEST",
    sampler="K_DPMPP_2M",  # DPM++ 2M Karras (https://stable-diffusion-art.com/samplers/)
    samples=1,
    seed=123234343,
    steps=25,
    style_preset=StylesPresets.ANIME.value,
    extras=None,
)

# Loop through all the frames in the source video or a selection of frames
for i in range(160, 200, 2):
    image_to_image_request(
        imageToImageRequest,
        f"{SOURCE_VIDEO_FRAMES}/frame{i}.jpg",
        GENERATED_VIDEO_FRAMES,
    )



(
    ffmpeg.input(
        f"{GENERATED_VIDEO_FRAMES}/*.jpg",
        pattern_type="glob",
        framerate=9,
    )
    .output(f"{GENERATED_VIDEO}")
    .run(overwrite_output=True)
)
```

